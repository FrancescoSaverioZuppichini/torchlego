{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchlego \n",
    "\n",
    "**This is an early preview**\n",
    "\n",
    "High quality Neural Networks built with reausable blocks in PyTorch\n",
    "\n",
    "![alt](https://raw.githubusercontent.com/FrancescoSaverioZuppichini/torchlego/develop/doc/images/lego.jpg)\n",
    "\n",
    "*Photo by Ryan Quintal on Unsplash*\n",
    "\n",
    "## Motivation\n",
    "\n",
    "This library aims to create new components to make developing and writing neural networks faster and easier.\n",
    "\n",
    "## Installation\n",
    "\n",
    "### pip\n",
    "You can install using pip\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/FrancescoSaverioZuppichini/torchlego\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks\n",
    "\n",
    "It follows a list of useful small components made to increase your code readibily and development. Just like lego, when combined, they can become anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convs\n",
    "Most of the times you will use $3x3$ conv or $1x1$ convs followed by *batchnorm* and an *activation function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchlego import conv3x3\n",
    "\n",
    "conv3x3(32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/conv3x3.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchlego import conv3x3_bn\n",
    "\n",
    "conv3x3_bn(32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/conv3x3_bn.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchlego import conv3x3_bn_act\n",
    "\n",
    "conv3x3_bn_act(32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/conv3x3_bn_act.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can always pass your own activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): SELU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import SELU\n",
    "conv3x3_bn_act(32, 64, act=SELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we have `conv1x1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchlego.blocks import conv1x1\n",
    "\n",
    "conv1x1(32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InputForward\n",
    "\n",
    "Many times we need to pass an input to multiple modules and do something with the results. This is went `InputForward` comes in handy! `aggr_func` takes as input a list where each entry is the output from one of the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchlego.blocks import InputForward, Lambda\n",
    "\n",
    "blocks = [Lambda(lambda x: x), Lambda(lambda x: x)]\n",
    "InputForward(blocks, aggr_func=lambda x: torch.tensor(x).sum())(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/InputForward.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cat\n",
    "\n",
    "An `InputForward` instance that concates the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchlego.blocks import Cat, Lambda\n",
    "\n",
    "\n",
    "blocks = [Lambda(lambda x: x), Lambda(lambda x: x)]\n",
    "\n",
    "Cat(blocks)(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Cat.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "Redisual connection are a big thing. They probably are most know from *resnet* paper (even if Schmidhuber did something very similar a long time ago). You can use `torchlego` to easily create **any** residual connection that you may need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual\n",
    "\n",
    "The main building block is the `Residual` class. Basically, it applys a function on the input and the output of a `blocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchlego.blocks import Residual, Lambda\n",
    "\n",
    "x = torch.tensor([1])\n",
    "block = Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "res = Residual(block, res_func=lambda x, res: x + res)\n",
    "res(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Residual.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shortcut\n",
    "We can also apply a function on the residual, this operation is called `shortcut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = Residual(block, res_func=lambda x, res: x + res, shortcut=lambda x: x * 2)\n",
    "res(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Residual_shorcut.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also not pass the `.res_func`, in that case the residual will be passed as second parameter to the `blocks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple lever residuals\n",
    "\n",
    "If you pass an array of `nn.ModuleList`, we assume you want to pass residual trought each respective layer. In the following example all the `A` layer just compute input + 1 the input while the `B` layers add the residual with the current input. Notice that the `B.forward` function takes as second argument the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class A(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x + 1\n",
    "    \n",
    "class B(nn.Module):\n",
    "    def forward(self, x, res):\n",
    "        return x + res\n",
    "        \n",
    "down = nn.ModuleList([A(), A()])\n",
    "up = nn.ModuleList([B(), B()])\n",
    "res = Residual([down, up])\n",
    "res(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Residual_blocks.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be awere that only the first `n` residuals will be passed, where `n` is the len of the second blocks. I know, it sound confusing, but let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class B(nn.Module):\n",
    "    def forward(self, x, res = None):\n",
    "        x = x if res is None else x + res\n",
    "        return x \n",
    "        \n",
    "down = nn.ModuleList([A(), A()])\n",
    "up = nn.ModuleList([B()])\n",
    "res = Residual([down, up])\n",
    "res(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Residual_blocks_no_last2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition\n",
    "\n",
    "`torchlego` comes with an useful `ResidualAdd` block that is just a `Residual` that performs automatically addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchlego.blocks import ResidualAdd, Lambda\n",
    "\n",
    "layer = ResidualAdd([Lambda(lambda x: x)])\n",
    "layer(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchlego.blocks import ResidualAdd\n",
    "\n",
    "x = torch.rand((1, 64, 8, 8))\n",
    "\n",
    "block = nn.Sequential(conv3x3_bn_act(64, 64, padding=1))\n",
    "\n",
    "layer = ResidualAdd([block])\n",
    "x = layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Add_block.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass multiple blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchlego.blocks import ResidualAdd, Lambda\n",
    "\n",
    "blocks = [Lambda(lambda x: x), Lambda(lambda x: x)]\n",
    "layer = ResidualAdd(blocks)\n",
    "layer(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Add_blocks.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a basic [ResNet](https://arxiv.org/abs/1512.03385) block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Residual(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (1): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchlego.blocks import conv_bn\n",
    "\n",
    "def resnet_basic_block(in_features, out_features):\n",
    "    shortbut = conv_bn(in_features, out_features, kernel_size=1, stride=2, bias=False) if in_features != out_features else nn.Identity()\n",
    "    stride = 2 if in_features != out_features else 1\n",
    "    return nn.Sequential(\n",
    "                ResidualAdd(nn.ModuleList([\n",
    "                    nn.Sequential(\n",
    "                        conv3x3_bn_act(in_features, out_features, stride=stride, padding=1, bias=False),\n",
    "                        conv3x3_bn(out_features, out_features, padding=1, bias=False))]), \n",
    "                    shortcut=shortbut),\n",
    "                nn.ReLU())\n",
    "    \n",
    "resnet_basic_block(32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/torchlego/blob/develop/doc/images/Add_resnet.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a full `resnet`? Easy peasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resnet(in_features, n_classes, sizes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_features, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        nn.Sequential(*[resnet_basic_block(64, 64) for _ in range(sizes[0])]),\n",
    "        resnet_basic_block(64, 128),\n",
    "        nn.Sequential(*[resnet_basic_block(128, 128) for _ in range(sizes[1] - 1)]),\n",
    "        resnet_basic_block(128, 256),\n",
    "        nn.Sequential(*[resnet_basic_block(256, 256) for _ in range(sizes[2] - 1)]),\n",
    "        resnet_basic_block(256, 512),\n",
    "        nn.Sequential(*[resnet_basic_block(512, 512) for _ in range(sizes[3] - 1)]),\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512, n_classes)\n",
    ")\n",
    "x = torch.rand((1,3,224,244))\n",
    "\n",
    "resnet34 = resnet(3, 1000, [3, 4, 6, 3])\n",
    "\n",
    "resnet34(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://www.researchgate.net/profile/Aaron_Vose/publication/330400293/figure/fig6/AS:715395283558403@1547574935970/ResNet-neural-network-architecture-ResNet-34-pictured-image-from-11.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Unet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1703,  0.1333,  0.1065,  ...,  0.0960,  0.0289,  0.2440],\n",
       "          [ 0.0541,  0.1370, -0.0009,  ..., -0.0234,  0.1766,  0.1227],\n",
       "          [ 0.0071,  0.0103, -0.0728,  ..., -0.0840,  0.0448,  0.0931],\n",
       "          ...,\n",
       "          [ 0.0387,  0.0116,  0.1317,  ...,  0.0861,  0.0679,  0.1378],\n",
       "          [ 0.0253,  0.0583, -0.0304,  ..., -0.0590, -0.0112,  0.0985],\n",
       "          [ 0.1501,  0.1165,  0.0767,  ...,  0.0516,  0.0438,  0.0853]],\n",
       "\n",
       "         [[ 0.0734,  0.1631, -0.0388,  ..., -0.0313, -0.0182, -0.0158],\n",
       "          [ 0.0150, -0.0157, -0.1313,  ...,  0.0417, -0.0827,  0.0312],\n",
       "          [-0.0783, -0.1002, -0.0104,  ..., -0.0625,  0.1027, -0.0133],\n",
       "          ...,\n",
       "          [-0.0326, -0.0716, -0.0284,  ..., -0.0891, -0.0317, -0.0103],\n",
       "          [ 0.1046, -0.0530, -0.0447,  ...,  0.0813, -0.0651,  0.0246],\n",
       "          [-0.0351,  0.0342, -0.0398,  ..., -0.0206, -0.0515,  0.0425]]]],\n",
       "       grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchlego.blocks import ResidualCat2d, Lambda\n",
    "from torchlego.blocks import Residual, conv3x3, conv3x3_bn_act, conv1x1\n",
    "\n",
    "down = lambda in_features, out_features: nn.Sequential(\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    conv3x3_bn_act(in_features, out_features, padding=1),\n",
    "    conv3x3_bn_act(out_features, out_features, padding=1),\n",
    ")\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_features, out_features, should_up=True, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_features, out_features, kernel_size=2, stride=2)\n",
    "        self.should_up = should_up\n",
    "        self.blocks =nn.Sequential(\n",
    "            conv3x3(out_features * 2, out_features, padding=1),\n",
    "            conv3x3(out_features, out_features, padding=1),\n",
    "    )\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        if self.should_up: x = self.up(x)\n",
    "            \n",
    "        diffX = x.size()[2] - res.size()[2]\n",
    "        diffY = x.size()[3] - res.size()[3]\n",
    "        pad = (diffX // 2, diffX // 2, diffY // 2, diffY // 2)\n",
    "        res = F.pad(res, pad)\n",
    "        \n",
    "        x = torch.cat([res, x], dim=1)\n",
    "        out = self.blocks(x)\n",
    "        return out\n",
    "\n",
    "unet = nn.Sequential(\n",
    "    Residual([\n",
    "    nn.ModuleList([\n",
    "        nn.Sequential(\n",
    "            conv3x3_bn_act(3, 64),\n",
    "            conv3x3_bn_act(64, 64)),\n",
    "        down(64, 128),\n",
    "        down(128, 256),\n",
    "        down(256, 512),\n",
    "        down(512, 1024),\n",
    "    ]),\n",
    "    nn.ModuleList([\n",
    "        up(512 * 2, 512),\n",
    "        up(256 * 2, 256),\n",
    "        up(128 * 2, 128),\n",
    "        up(64 * 2, 64),\n",
    "\n",
    "    ])\n",
    "]),         \n",
    "    conv1x1(64, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand((1,3,256,256))\n",
    "\n",
    "unet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- [ ] if second number is not passed in conv just use the first one\n",
    "- [ ] in all the sequential use OrderDict to name them\n",
    "- [ ] little doc for module transfer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
